includes:
  - path/to/model/config.yml

name: multimodalvqa_medvqa2019
ckpt_dir: ckpt

dataset:
  proto: MedVQA_2019_Dataset
  imgpath: /home/faith/MVLM/image_clef/images
  tokenizer: allenai/biomed_roberta_base
  max_words: 32

transforms:
  type: VQATransforms

model:
  proto: MultimodalVQA

  encoder:
    proto: allenai/biomed_roberta_base

  fusion:
    input_text_embed_size: 768
    # input_image_embed_size: 2048 # overwritten
    hidden_size: 768
    num_top_layer: 6
    vocab_size: 50265
    num_layers: 6
    num_heads: 12
    max_text_len: 32 # same as max_words under tokenizer
    drop_rate: 0.1
    mlp_ratio: 4

  classifier:
    num_classes: 1730

  loss:
    proto: BCEWithLogitsLoss

trainer:
  optimizer: AdamW
  optim_params:
    optim_groups: heads
    lr: 5e-6
    eps: 1e-8
    betas: (0.9, 0.98)
    weight_decay: 5e-4
    lr_multiplier_head: 50
    lr_multiplier_multi_modal: 5
  batch_size: 16
  lr_decay: get_polynomial_decay_schedule_with_warmup
  lr_decay_params:
    num_warmup_steps: 1000 # 0.1*10000
    num_training_steps: 10000
    lr_end: 0
    power: 1
  epochs: 50
  early_stop: 10
  eval_start: 0
  early_stop_metric: vqa_score

validator:
  batch_size: 16
  metrics: [vqa_score]
  splits: [valid]
