includes:
  - config/templates/finetune/models/gloria.yml

name: GLoRIA
ckpt_dir: experiments/classification/covidx/
use_amp: True
seed: 42

dataset:
  proto: COVIDx_Dataset
  data_path: /mnt/weka/MedFM/processed_data/COVIDx-CXR4
  csvpath: /mnt/weka/MedFM/processed_data/COVIDx-CXR4/covidx_labels.csv
  split: "train_1"
  num_workers: 16

transforms:
  type: NIHTransforms

pretrained_source: custom
pretrained_path:
  official: /mnt/weka/MedFM/BenchX/ckpt/official/GLoRIA.ckpt
  custom: /mnt/weka/MedFM/BenchX/ckpt/pretrained/GLoRIA_last.ckpt
pretrained_prefix:
  official: gloria.img_encoder.model.
  custom: gloria.img_encoder.model.

model:
  proto: ImageClassifier

  cnn:
    output_layer: avgpool
    pretrained: ${pretrained_path.${pretrained_source}}
    prefix: ${pretrained_prefix.${pretrained_source}}
    freeze: False

  classifier:
    proto: Classifier
    num_classes: 2
    use_fc_norm: True
    # trunc_init: True
    dropout: 0.

  loss:
    proto: CrossEntropyLoss

trainer:
  optimizer: Adam
  optim_params:
    lr: 5e-4
    eps: 1e-08
    optim_groups: ve_only
    lr_multiplier_ve: 0.1
    # weight_decay: 1e-5
  batch_size: 32
  clip_grad_norm: 1.0
  lr_decay: WarmupCosineScheduler
  lr_decay_params:
    warmup_steps: 50
    t_total: 3000
  epochs: 200
  early_stop: 10
  eval_start: 10
  eval_interval: 5
  early_stop_metric: multiclass_f1

validator:
  batch_size: 512
  metrics: [multiclass_accuracy, multiclass_precision, multiclass_recall, multiclass_f1, multiclass_auroc]
  splits: [val]