includes:
  - config/templates/finetune/models/mrm.yml

name: MRM
ckpt_dir: experiments/classification/chexpert/
use_amp: True
seed: 42

dataset:
  proto: CheX_Dataset
  data_path: /home/users/astar/ares/zhouy2/scratch/processed_data/CheXpert
  csvpath: /home/users/astar/ares/zhouy2/scratch/processed_data/CheXpert/chexpert_labels.csv
  views: ["PA", "AP"]
  split: "train_1"
  num_workers: 10

transforms:
  type: NIHTransforms

pretrained_source: official
pretrained_path:
  official: /home/users/astar/ares/zhouy2/scratch/BenchX/ckpt/official/MRM.pth
  custom: /home/users/astar/ares/zhouy2/scratch/BenchX/ckpt/pretrained/MRM.pth

model:
  proto: ImageClassifier

  cnn:
    num_classes: 0  # disable classifier layer
    # output_layer: avgpool
    global_pool: avg
    pretrained: ${pretrained_path.${pretrained_source}}
    drop_path_rate: 0.
    permute: no_permute
    # permute: batch_first
    freeze: False

  classifier:
    # proto: Identity
    proto: Classifier
    # use_fc_norm: True
    num_classes: 13
    use_fc_norm: True
    trunc_init: True

  loss:
    proto: BCEWithLogitsLoss

trainer:
  optimizer: SGD
  optim_params:
    lr: 1e-2
    momentum: 0.9
    optim_groups: ve_only
    lr_multiplier_ve: 0.1
    # weight_decay: 1e-5
    # weight_decay: 0
  batch_size: 64
  clip_grad_norm: 1.0
  lr_decay: WarmupCosineScheduler
  lr_decay_params:
    warmup_steps: 50
    t_total: 3000
  epochs: 200
  early_stop: 10
  eval_start: 10
  eval_interval: 5
  early_stop_metric: multilabel_auroc

validator:
  batch_size: 512
  metrics: [multilabel_accuracy, multilabel_auroc]
  # splits: [valid]
  splits: [val]